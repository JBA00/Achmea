import pandas as pd
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import seaborn as sns

def create_df(what_for="clustering"):
    # Create the data frames
    erasmus_db = pd.read_csv("Erasmus.csv", sep=";")
    mutation_log_db = pd.read_csv("Mutatielog_id.csv", sep=";")
    toelichting_db = pd.read_excel("Toelichting.xlsx")

    # Create new columns for date and time
    mutation_log_db['DatumRegistratie'] = pd.to_datetime(
        mutation_log_db['DatumRegistratie'])

    mutation_log_db['date'] = mutation_log_db['DatumRegistratie'].dt.date
    mutation_log_db['time'] = mutation_log_db['DatumRegistratie'].dt.time

    # Fix float values.
    for col in erasmus_db.columns:
        try:
            erasmus_db[col] = erasmus_db[col].str.replace(
                ',', '.').astype(float)
        except Exception:
            continue

    # Translate the column into lists, so that maybe we can calculate the number of red flags.
    erasmus_db['SF_woord'] = erasmus_db['SF_woord'].str.replace(
        ' ', '').str.split(',')

    erasmus_db['SF_woord_count'] = [len(x) if isinstance(
        x, list) else None for x in erasmus_db['SF_woord']]

    if what_for == "random_forest":
        erasmus_db = erasmus_db.drop("SF_woord", axis=1).drop(
            "id", axis=1).drop("prediction", axis=1)

    erasmus_db["SF_woord_count"].fillna(0, inplace=True)

    # Here are some assumptions about are made, to replace null values- probs better to discuss.
    erasmus_db["woord_74"].fillna(0, inplace=True)
    erasmus_db["woord_121"].fillna(0, inplace=True)

    erasmus_db["D_LG4_1j"].fillna(0, inplace=True)
    erasmus_db["D_cat_1g"].fillna(0, inplace=True)
    erasmus_db["D_cat_4b"].fillna(0, inplace=True)

    erasmus_db["VT_cat_1b"].fillna(999, inplace=True)
    erasmus_db["VT_cat_1d"].fillna(999, inplace=True)
    erasmus_db["VT_cat_1g"].fillna(999, inplace=True)
    erasmus_db["VT_cat_2c"].fillna(999, inplace=True)
    erasmus_db["VT_cat_4b"].fillna(999, inplace=True)
    erasmus_db["VT_cat_5e"].fillna(999, inplace=True)
    erasmus_db["VT_cat_8a"].fillna(999, inplace=True)
    erasmus_db["VT_cat_9a"].fillna(999, inplace=True)
    erasmus_db["VT_cat_10b"].fillna(999, inplace=True)
    erasmus_db["VT_cat_10g.2"].fillna(999, inplace=True)

    if what_for == "eda":
        erasmus_db = erasmus_db[["vm", "polis_2", "polis_5",
                                 "age", "status", "LG1", "SF_woord_count"]]
    if what_for=="clustering":
        erasmus_db = erasmus_db.drop(["SF_woord","prediction"], axis=1)    

    return erasmus_db


# Clustering
df_clustering = create_df("clustering")
print(df_clustering)


# Create dummy variables for status
df_clustering = pd.get_dummies(df_clustering, columns=['status'])
print(df_clustering.head())

# Create bolean values for VT Features
columns_to_replace = ['VT_LG3_2_1', 'VT_LG4_1a', 'VT_LG4_1d', 'VT_LG4_1e', 'VT_LG4_1b',
       'VT_LG2_2_2', 'VT_cat_1b', 'VT_cat_1d', 'VT_cat_1g', 'VT_cat_1he',
       'VT_cat_2a', 'VT_cat_2b', 'VT_cat_2c', 'VT_cat_2f', 'VT_cat_2k',
       'VT_cat_3b', 'VT_cat_3e', 'VT_cat_4a', 'VT_cat_4b', 'VT_cat_4f',
       'VT_cat_4hd', 'VT_cat_4he', 'VT_cat_5e', 'VT_cat_5gc', 'VT_cat_6b',
       'VT_cat_7a', 'VT_cat_8a', 'VT_cat_9a', 'VT_cat_10a', 'VT_cat_10b',
       'VT_cat_10c', 'VT_cat_10f', 'VT_cat_10g', 'VT_cat_10g.1',
       'VT_cat_10g.2', 'VT_cat_10g.3', 'VT_cat_10g.4', 'VT_cat_11d',
       'VT_EV_1_1', 'VT_EV_2_1', 'VT_EV_3_1', 'VT_EV_4_1', 'VT_EV_6_1',
       'VT_EV_7_1_3', 'VT_EV_8_1_3']

# Replace "999" with False and all other values with True
df_clustering[columns_to_replace] = df_clustering[columns_to_replace] == '999'
print(df_clustering.head())

#find numerical, and boolean features 
numerical_features = df_clustering.select_dtypes(include=['int64', 'float64']).columns
boolean_features = df_clustering.select_dtypes(include=['bool']).columns
print(numerical_features)
print(boolean_features)

# Turn Boolean features to binary
def binary_transform(value, desired_value=True):
    return 1 if value == desired_value else 0

df_clustering[boolean_features] = df_clustering[boolean_features].applymap(binary_transform)
print(df_clustering)

# Exclude vm and sf from numerical features 
features_to_exclude = ['vm', 'SF', 'id']
numerical_features = numerical_features.difference(features_to_exclude)

# Normalize numerical features
scaler = MinMaxScaler()
df_clustering[numerical_features] = scaler.fit_transform(df_clustering[numerical_features])


# Cluster and find centroids for all features
num_clusters = 2
kmeans = KMeans(n_clusters=num_clusters, random_state=1234)
df_clustering['cluster_label'] = kmeans.fit_predict(df_clustering.drop('id', axis=1))

# Add 'id' column back to the DataFrame

df_clustering_with_clusters = pd.concat([df_clustering[['id']], df_clustering.drop('id', axis=1)], axis=1)

# Get the count of IDs in each cluster
cluster_counts = df_clustering_with_clusters['cluster_label'].value_counts()
cluster_ids_dict = {}

print(df_clustering)

for cluster_label in range(num_clusters):
    print(f"\nCluster {cluster_label + 1} Features with Biggest Effect:")
    
    # Get IDs for the specific cluster
    cluster_ids = df_clustering_with_clusters.loc[df_clustering_with_clusters['cluster_label'] == cluster_label, 'id']
    
    # Store IDs in the dictionary
    cluster_ids_dict[f'Cluster_{cluster_label + 1}'] = cluster_ids.tolist()
    
    # Print the count of IDs in the cluster
    #print(f"Count of IDs in Cluster {cluster_label + 1}: {cluster_counts[cluster_label]}")
    
    # Print IDs
    #print(f"IDs in Cluster {cluster_label + 1}: {cluster_ids.tolist()}")
    
    # Print top 5 features for the cluster
    abs_centroids = abs(df_clustering.loc[df_clustering['cluster_label'] == cluster_label, :].drop(['id', 'cluster_label'], axis=1).mean())
   
    sorted_features = abs_centroids.sort_values(ascending=False)
    
    top_features = sorted_features.index[:5]  
    for feature in top_features:
        feature_value = df_clustering.loc[df_clustering['cluster_label'] == cluster_label, feature].mean()
        print(f"- {feature}: {feature_value}")

# Access the IDs for each cluster using the dictionary
#for cluster_name, ids in cluster_ids_dict.items():
    #print(f"\nIDs in {cluster_name}: {ids}")
    


# Heat map for clusters
# Data for Cluster 1
cluster1_data = {
    'EV_6': 0.9861564235958028,
    'EV_3': 0.9481527202186756,
    'EV_8': 0.9236398906621991,
    'cat_1i': 0.903976721629486,
    'cat_4i': 0.8896922669958558
,
}

# Data for Cluster 2
cluster2_data = {
    'cat_4i': 0.9725622625580413,
    'cat_1i': 0.9651329674968341,
    'cat_8h': 0.9517940059096666,
    'cat_6g': 0.9481637821865766,
    'cat_7f': 0.938117349092444,
}

# Create a DataFrame
df_cluster2 = pd.DataFrame(list(cluster2_data.items()), columns=['Features', 'Cluster 2'])
df_cluster1 = pd.DataFrame(list(cluster1_data.items()), columns=['Features', 'Cluster 1'])


# Pivot the data for heatmap
heatmap_data = pd.concat([df_cluster1.set_index('Features'), df_cluster2.set_index('Features')], axis=1).T

# Create the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, cmap='viridis', annot=True, fmt=".3f", linewidths=.5)
plt.title('Feature Effects in Clusters')
plt.show()

# Save the DataFrame to an Excel file
#cluster_ids_df = pd.DataFrame.from_dict(cluster_ids_dict, orient='index').transpose()
#cluster_ids_df.to_excel("cluster_ids.xlsx", index=False)
